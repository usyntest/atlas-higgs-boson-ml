{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "J1nlIkkE2P20"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# basic imports\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz -o atlas-higgs-challenge-2014-v2.csv.gz\n",
        "!gunzip -f atlas-higgs-challenge-2014-v2.csv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ6-hTVL67rc",
        "outputId": "29d0c5d9-f82a-4924-a036-13d4b9bb61be"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 62.5M  100 62.5M    0     0  9939k      0  0:00:06  0:00:06 --:--:-- 15.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv')\n",
        "\n",
        "df['Label'] = df['Label'].map({'b': 0, 's': 1})\n",
        "\n",
        "training_mask = df['KaggleSet'] == 't'\n",
        "test_mask = df['KaggleSet'] == 'b'\n",
        "\n",
        "feature_columns = [\n",
        "    'DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h',\n",
        "    'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet',\n",
        "    'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau',\n",
        "    'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt',\n",
        "    'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi',\n",
        "    'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
        "    'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
        "    'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt',\n",
        "    # 'Weight'\n",
        "]\n",
        "\n",
        "X_train = torch.FloatTensor(df[training_mask][feature_columns].values)\n",
        "y_train = torch.FloatTensor(df[training_mask]['Label'].values)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "X_test = torch.FloatTensor(df[test_mask][feature_columns].values)\n",
        "y_test = torch.FloatTensor(df[test_mask]['Label'].values)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "nQR5M57569WN"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a model based on the given architecture\n",
        "def create_model(architecture, input_dim=30):\n",
        "    layers = []\n",
        "    for units in architecture:\n",
        "        layers.append(nn.Linear(input_dim, units))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        input_dim = units\n",
        "    layers.append(nn.Linear(input_dim, 1))  # Output layer\n",
        "    layers.append(nn.Sigmoid())  # Ensure output is in [0,1]\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "YdP7xwLm2aGx"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ensemble architectures\n",
        "architectures = [\n",
        "    [50, 1],\n",
        "    [50, 25, 1],\n",
        "    [50, 50, 25, 1]\n",
        "]\n",
        "\n",
        "# Create 108 models in total (36 models for each architecture)\n",
        "ensemble_models = [create_model(arch) for arch in architectures for _ in range(36)]"
      ],
      "metadata": {
        "id": "fd3Ux9d43Q6O"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train a single model\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=50):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Squeeze the outputs to match the shape of targets\n",
        "            outputs = outputs.squeeze()  # Shape: [batch_size, 1] -> [batch_size]\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "QAqfiBTp3try"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make predictions using the ensemble\n",
        "def ensemble_predict(models, data_loader, device):\n",
        "    with torch.no_grad():\n",
        "        predictions = torch.zeros(len(models), len(data_loader.dataset))\n",
        "        for i, model in enumerate(models):\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            for j, (inputs, _) in enumerate(data_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs).squeeze()  # Squeeze the outputs\n",
        "                predictions[i, j * inputs.size(0):(j + 1) * inputs.size(0)] = outputs.cpu()\n",
        "        return predictions.mean(dim=0)"
      ],
      "metadata": {
        "id": "rtQlKNrk4kbg"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizers\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in ensemble_models]"
      ],
      "metadata": {
        "id": "-xabT9fh6L9R"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'"
      ],
      "metadata": {
        "id": "WFDuEbSH-oFp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model, optimizer in zip(ensemble_models, optimizers):\n",
        "    train_model(model, train_loader, criterion, optimizer, device=device, num_epochs=50)"
      ],
      "metadata": {
        "id": "-vnCeTNF6q1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = ensemble_predict(ensemble_models, test_loader, device=device)\n",
        "print(\"Ensemble predictions:\", predictions)"
      ],
      "metadata": {
        "id": "PS22YZ36_6er"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}