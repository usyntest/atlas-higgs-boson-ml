{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWeYzdED/aypP6EwBhxmAj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Imports and Dataset Download"],"metadata":{"id":"7LNO38uUVe_h"}},{"cell_type":"code","source":["!pip install -q pytorch-tabular"],"metadata":{"id":"jyAtxxLYER6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset preprocessing and metric imports\n","from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from imblearn.over_sampling import SMOTE\n","from sklearn.decomposition import PCA\n","from sklearn.utils import shuffle\n","\n","# xgboost imports\n","from xgboost import XGBClassifier\n","from xgboost import plot_importance\n","\n","# deep learning import\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# basic imports\n","import matplotlib.pyplot as plt\n","from typing import Tuple\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","%matplotlib inline"],"metadata":{"id":"WmvHxy8G3-zI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDwS827y3PAn","executionInfo":{"status":"ok","timestamp":1737574574376,"user_tz":-330,"elapsed":14610,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"f97ee0ac-e05e-4a28-fe68-e576fcd4ec3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 62.5M  100 62.5M    0     0  7161k      0  0:00:08  0:00:08 --:--:-- 9243k\n"]}],"source":["!curl https://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz -o atlas-higgs-challenge-2014-v2.csv.gz\n","!gunzip -f atlas-higgs-challenge-2014-v2.csv.gz"]},{"cell_type":"markdown","source":["## Data Preparation"],"metadata":{"id":"al3QLT-GQeBE"}},{"cell_type":"code","source":["df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv')\n","feature_columns = [\n","    'DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h',\n","    'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet',\n","    'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau',\n","    'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt',\n","    'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi',\n","    'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n","    'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n","    'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt'\n","]"],"metadata":{"id":"U0XLBf3B4Jz3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["label encoding"],"metadata":{"id":"n8rifsIRaf1f"}},{"cell_type":"code","source":["df['Label'] = df['Label'].map({'b': 0, 's': 1})\n","training_data = df[df['KaggleSet'] == 't']\n","test_data = df[df['KaggleSet'] == 'b']"],"metadata":{"id":"hNH7rmoAQlTm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["missing values filled"],"metadata":{"id":"sIUSB5OlORui"}},{"cell_type":"code","source":["training_data = training_data.replace(-999.0, np.nan)\n","training_data[feature_columns] = training_data[feature_columns].fillna(training_data[feature_columns].median())"],"metadata":{"id":"qODTlnTXOQ5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["feature scaling"],"metadata":{"id":"dpIDhbF0WUrc"}},{"cell_type":"code","source":["scaler = StandardScaler()\n","x_scaled = scaler.fit_transform(training_data[feature_columns])\n","x_scaled = pd.DataFrame(x_scaled, columns=feature_columns)"],"metadata":{"id":"eWIfr2SmP2DB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["feature engineering"],"metadata":{"id":"sM-DqR-JayZO"}},{"cell_type":"code","source":["pca = PCA(n_components=20, random_state=42)  # Reduce to 20 principal components\n","x_pca = pca.fit_transform(x_scaled)"],"metadata":{"id":"PlDIt719az2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["balancing the dataset"],"metadata":{"id":"YMpWYUHQYHG9"}},{"cell_type":"code","source":["# smote = SMOTE(random_state=42)\n","# X_smote, y_smote = smote.fit_resample(x_pca, training_data['Label'])"],"metadata":{"id":"DwKoJTwgWcbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X, y = shuffle(x_pca, y_smote, random_state=42)\n","data = pd.DataFrame(x_pca, columns=[f'PC{i+1}' for i in range(x_pca.shape[1])])\n","data['Label'] = training_data['Label']\n","data['Weight'] = training_data['Weight']\n","\n","train, val = train_test_split(data, test_size=0.2, random_state=42)\n","\n","print(\"Data Shape:\", data.shape)\n","print(\"0 Class Instances:\", data['Label'].value_counts()[0])\n","print(\"1 Class Instances:\", data['Label'].value_counts()[1])\n","\n","print(\"Training Data Shape:\", train.shape)\n","print(\"Validation Data Shape:\", val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85Gu5wUEeauN","executionInfo":{"status":"ok","timestamp":1737570811359,"user_tz":-330,"elapsed":17,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"81d20742-e518-454b-bd00-0e394c1453d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Shape: (250000, 22)\n","0 Class Instances: 164333\n","1 Class Instances: 85667\n","Training Data Shape: (200000, 22)\n","Validation Data Shape: (50000, 22)\n"]}]},{"cell_type":"code","source":["train_weights = train['Weight']\n","val_weights = val['Weight']\n","X_train = train.drop(columns=['Label', 'Weight'], axis=1)\n","y_train = train['Label']\n","X_val = val.drop(columns=['Label', 'Weight'], axis=1)\n","y_val = val['Label']"],"metadata":{"id":"yXpeXXsIFpQY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Weighted AMS Score"],"metadata":{"id":"hHriXweuNOHj"}},{"cell_type":"code","source":["def weighted_ams_score(y_true, y_pred, weights, b_reg=10):\n","    \"\"\"\n","    Calculate AMS score using event weights\n","\n","    Parameters:\n","    -----------\n","    y_true : array-like\n","        True labels (0 for background, 1 for signal)\n","    y_pred : array-like\n","        Predicted labels (0 for background, 1 for signal)\n","    weights : array-like\n","        Event weights for each observation\n","    b_reg : float\n","        Regularization term for background\n","    \"\"\"\n","    # Calculate weighted signal (s) and background (b)\n","    s = np.sum(weights[(y_true == 1) & (y_pred == 1)])  # True positives weighted\n","    b = np.sum(weights[(y_true == 0) & (y_pred == 1)])  # False positives weighted\n","\n","    # Calculate AMS\n","    return np.sqrt(2 * ((s + b + b_reg) * np.log(1 + s/(b + b_reg)) - s))\n","\n","def evaluate_weighted_predictions(y_true, y_pred, weights):\n","    \"\"\"\n","    Evaluate predictions with weights and print detailed statistics\n","    \"\"\"\n","    # Calculate weighted counts\n","    weighted_tp = np.sum(weights[(y_true == 1) & (y_pred == 1)])\n","    weighted_fp = np.sum(weights[(y_true == 0) & (y_pred == 1)])\n","    weighted_tn = np.sum(weights[(y_true == 0) & (y_pred == 0)])\n","    weighted_fn = np.sum(weights[(y_true == 1) & (y_pred == 0)])\n","\n","    print(\"Weighted Prediction Statistics:\")\n","    print(f\"Weighted True Positives: {weighted_tp:.2f}\")\n","    print(f\"Weighted False Positives: {weighted_fp:.2f}\")\n","    print(f\"Weighted True Negatives: {weighted_tn:.2f}\")\n","    print(f\"Weighted False Negatives: {weighted_fn:.2f}\")\n","\n","    # Calculate weighted metrics\n","    weighted_precision = weighted_tp / (weighted_tp + weighted_fp)\n","    weighted_recall = weighted_tp / (weighted_tp + weighted_fn)\n","\n","    print(f\"\\nWeighted Precision: {weighted_precision:.4f}\")\n","    print(f\"Weighted Recall: {weighted_recall:.4f}\")\n","\n","    # Calculate AMS\n","    ams = weighted_ams_score(y_true, y_pred, weights)\n","    print(f\"\\nWeighted AMS Score: {ams:.4f}\")\n","\n","    return ams"],"metadata":{"id":"3DaunEEZNNOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XGBoost Model"],"metadata":{"id":"Fc3KsIGY9DMB"}},{"cell_type":"code","source":["xgb_model = XGBClassifier(\n","    n_estimators=500,\n","    max_depth=6,\n","    learning_rate=0.01,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    min_child_weight=1,\n","    gamma=1,\n","    reg_alpha=0.1,\n","    reg_lambda=1,\n","    # scale_pos_weight=99,  # Adjust based on your class imbalance\n","    random_state=42,\n","    tree_method='hist',\n","    eval_metric=['auc', 'aucpr'],\n","    early_stopping_rounds=20\n",")\n","\n","xgb_model.fit(\n","    X_train,\n","    y_train,\n","    eval_set=[(X_train, y_train), (X_val, y_val)],\n","    verbose=100\n",")\n","\n","print('xgb model trained')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-3RlGwK4m8a","executionInfo":{"status":"ok","timestamp":1737570884024,"user_tz":-330,"elapsed":72678,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"3ad29cd7-4116-4fc1-94cc-9b05b5b184e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-auc:0.73879\tvalidation_0-aucpr:0.01136\tvalidation_1-auc:0.74132\tvalidation_1-aucpr:0.01106\n","[100]\tvalidation_0-auc:0.79724\tvalidation_0-aucpr:0.02032\tvalidation_1-auc:0.79547\tvalidation_1-aucpr:0.01956\n","[200]\tvalidation_0-auc:0.85332\tvalidation_0-aucpr:0.02536\tvalidation_1-auc:0.85162\tvalidation_1-aucpr:0.02688\n","[300]\tvalidation_0-auc:0.87481\tvalidation_0-aucpr:0.02808\tvalidation_1-auc:0.87321\tvalidation_1-aucpr:0.02772\n","[400]\tvalidation_0-auc:0.88464\tvalidation_0-aucpr:0.03026\tvalidation_1-auc:0.88208\tvalidation_1-aucpr:0.02924\n","[499]\tvalidation_0-auc:0.89518\tvalidation_0-aucpr:0.03265\tvalidation_1-auc:0.89246\tvalidation_1-aucpr:0.03143\n","xgb model trained\n"]}]},{"cell_type":"markdown","source":["## DNN Model"],"metadata":{"id":"UrinZjM98-_j"}},{"cell_type":"code","source":["class TabularDNN(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(128, 64),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","\n","            nn.Linear(64, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","def train_model(X_train, y_train, X_val, y_val, epochs=50):\n","    # Get correct input dimension (number of features)\n","    input_dim = X_train.shape[1]\n","\n","    # Convert to tensors\n","    X_train = torch.FloatTensor(X_train)\n","    y_train = torch.FloatTensor(y_train)\n","    X_val = torch.FloatTensor(X_val)\n","    y_val = torch.FloatTensor(y_val)\n","\n","    # Create dataloaders\n","    train_dataset = TensorDataset(X_train, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n","\n","    # Initialize model and training components\n","    model = TabularDNN(input_dim)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n","    criterion = nn.BCELoss()\n","\n","    best_auc = 0\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_losses = []\n","\n","        for X_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            y_pred = model(X_batch).squeeze()\n","            loss = criterion(y_pred, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_losses.append(loss.item())\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_preds = model(X_val).squeeze()\n","            val_auc = roc_auc_score(y_val, val_preds.numpy())\n","\n","        scheduler.step(val_auc)\n","\n","        if val_auc > best_auc:\n","            best_auc = val_auc\n","            torch.save(model.state_dict(), 'best_model.pt')\n","\n","        if epoch % 5 == 0:\n","            print(f'Epoch {epoch}: Loss = {np.mean(epoch_losses):.4f}, Val AUC = {val_auc:.4f}')\n","\n","    return model"],"metadata":{"id":"eJM8eDhCE-7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = train_model(X_train.to_numpy(), y_train.to_numpy(), X_val.to_numpy(), y_val.to_numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vCxIeXJ2teZ","executionInfo":{"status":"ok","timestamp":1737571199699,"user_tz":-330,"elapsed":315693,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"a78772d9-4355-4332-b57c-3aea700aaacd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Loss = 0.0632, Val AUC = 0.8596\n","Epoch 5: Loss = 0.0050, Val AUC = 0.9201\n","Epoch 10: Loss = 0.0049, Val AUC = 0.9277\n","Epoch 15: Loss = 0.0048, Val AUC = 0.9302\n","Epoch 20: Loss = 0.0048, Val AUC = 0.9312\n","Epoch 25: Loss = 0.0047, Val AUC = 0.9323\n","Epoch 30: Loss = 0.0047, Val AUC = 0.9337\n","Epoch 35: Loss = 0.0047, Val AUC = 0.9342\n","Epoch 40: Loss = 0.0046, Val AUC = 0.9341\n","Epoch 45: Loss = 0.0046, Val AUC = 0.9345\n"]}]},{"cell_type":"markdown","source":["## Ensemble of DNN and XGBoost"],"metadata":{"id":"OtT1xXhX5X8i"}},{"cell_type":"code","source":["def preprocess_test_data(X_test, scaler, pca):\n","    X_test = X_test.replace(-999.0, np.nan)\n","    X_test[feature_columns] = X_test[feature_columns].fillna(X_test[feature_columns].median())\n","    X_scaled = scaler.transform(X_test)\n","    X_processed = pca.transform(X_scaled)\n","    return X_processed"],"metadata":{"id":"3cBewtfo5XSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ensemble_predict(xgb_model, dnn_model, test_df, scaler, feature_cols, pca=None, threshold=0.5, weights=[0.5, 0.5]):\n","    X_processed = preprocess_test_data(test_df, scaler, feature_cols, pca)\n","    xgb_probs = xgb_model.predict_proba(X_processed)[:, 1]\n","\n","    X_torch = torch.FloatTensor(X_processed)\n","    dnn_model.eval()\n","    with torch.no_grad():\n","        dnn_probs = dnn_model(X_torch).squeeze().numpy()\n","\n","    # Combine\n","    ensemble_probs = weights[0] * xgb_probs + weights[1] * dnn_probs\n","    ensemble_preds = (ensemble_probs > threshold).astype(int)\n","\n","    return ensemble_preds, ensemble_probs"],"metadata":{"id":"Ryy6n1Eb_8xX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = test_data[feature_columns]\n","y_test = test_data['Label']\n","weights = test_data['Weight']\n","\n","ensemble_preds, probs = ensemble_predict(\n","    xgb_model, model, X_test,\n","    scaler=scaler,\n","    pca=pca,\n","    weights=[0.5, 0.5]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRjjZlsbA5Hz","executionInfo":{"status":"ok","timestamp":1737571201194,"user_tz":-330,"elapsed":1501,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"42d909b7-fbd9-4164-d693-4f763ebd6dc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["evaluate_weighted_predictions(y_test, ensemble_preds, weights)\n","print(f\"\\n\\nPrecision: {precision_score(y_test, ensemble_preds)}, Recall: {recall_score(y_test, ensemble_preds)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kg_CwZc1B0eh","executionInfo":{"status":"ok","timestamp":1737571201194,"user_tz":-330,"elapsed":5,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"ce1b64e1-58e9-4eb8-b2d7-4a268f1bebc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Weighted Prediction Statistics:\n","Weighted True Positives: 0.00\n","Weighted False Positives: 0.00\n","Weighted True Negatives: 50463.91\n","Weighted False Negatives: 84.25\n","\n","Weighted Precision: nan\n","Weighted Recall: 0.0000\n","\n","Weighted AMS Score: 0.0000\n","\n","\n","Precision: 0.0, Recall: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-171-900da544d6c5>:40: RuntimeWarning: invalid value encountered in scalar divide\n","  weighted_precision = weighted_tp / (weighted_tp + weighted_fp)\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":["ensemble_preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23K65YUuB5wW","executionInfo":{"status":"ok","timestamp":1737571646546,"user_tz":-330,"elapsed":6,"user":{"displayName":"Uday Sharma","userId":"16062094916337062773"}},"outputId":"790a9da3-3cc4-4963-e911-e5b21872c458"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 0, 0, 0])"]},"metadata":{},"execution_count":179}]}]}